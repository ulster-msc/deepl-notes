![Question 15](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.33.23.png?raw=true)

Question 15 — Compute z and ŷ(z)

Detailed explanation

**Understanding the Neural Network Unit:**

The diagram shows a simple perceptron with:
- Input vector: x = [x₁, x₂, x₃, x₄]
- Weight vector: w = [w₁, w₂, w₃, w₄]
- Bias: b₀ with its own weight w₀
- Pre-activation sum: z
- Activation function: ŷ (appears to be sigmoid based on context)

**Given values:**
- b₀ = 0.5 (bias value)
- w₀ = 1 (bias weight)
- x = [2, 1, 0, 1]ᵀ (input vector)
- w = [0.2, -0.1, 0.3, 0.4]ᵀ (weight vector)

**Step 1: Compute the weighted sum z**

The pre-activation value z is computed as:
```
z = (bias term) + (weighted sum of inputs)
z = b₀ × w₀ + Σ(xᵢ × wᵢ)
```

**Calculate each component:**

**Bias term:**
- b₀ × w₀ = 0.5 × 1 = 0.5

**Weighted inputs (element-wise multiplication):**
- x₁ × w₁ = 2 × 0.2 = 0.4
- x₂ × w₂ = 1 × (-0.1) = -0.1
- x₃ × w₃ = 0 × 0.3 = 0.0
- x₄ × w₄ = 1 × 0.4 = 0.4

**Sum the weighted inputs:**
- Σ(xᵢ × wᵢ) = 0.4 + (-0.1) + 0.0 + 0.4 = 0.4 - 0.1 + 0.4 = 0.7

**Total z:**
- z = 0.5 + 0.7 = **1.2**

**Step 2: Apply activation function ŷ = σ(z)**

The activation function is sigmoid:
```
σ(z) = 1 / (1 + e^(-z))
```

**Calculate σ(1.2):**
- e^(-1.2) = e^(-1.2) ≈ 0.30119
- 1 + e^(-1.2) ≈ 1 + 0.30119 = 1.30119
- σ(1.2) = 1 / 1.30119 ≈ 0.7685

Therefore: ŷ ≈ **0.7685** (or **0.77** rounded to 2 decimal places)

**Final answers:**
- **z = 1.2**
- **ŷ ≈ 0.7685**

References (lectures/practicals used)
- practicals/Pratice -W2 solution.pdf — p.1, p.3, p.9–10 (sigmoid function and perceptron forward pass)
- lectures/Lecture 2 - 2025.pdf — p.2 (sigmoid curve), p.5 (linear combination + activation context)
