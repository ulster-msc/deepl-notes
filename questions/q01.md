![Question 1](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.31.18.png?raw=true)

Question 1 — One‑hot vector size

- Final answer: 4

Detailed explanation

**Understanding One-Hot Encoding:**
One-hot encoding is a fundamental technique in Natural Language Processing (NLP) and machine learning for representing categorical data (like words) as numerical vectors. In this representation:
- Each unique category (word) gets its own dimension in the vector space
- Only one dimension has the value 1 (hence "one-hot"), all others are 0
- The position of the 1 identifies which category/word it represents

**Step-by-step solution:**

1. **Count the vocabulary size:**
   The question provides a vocabulary containing these words:
   - "great"
   - "terrible"
   - "fantastic"
   - "boring"

   Total distinct words = 4

2. **Apply the one-hot encoding rule:**
   In one-hot encoding, the dimensionality (length) of each word vector MUST equal the total vocabulary size.
   - Vocabulary size = 4 words
   - Therefore, vector size = 4 dimensions

3. **Why this size?**
   Each word needs its own unique position in the vector. With 4 words, we need exactly 4 positions to uniquely identify each one.

**Example encodings (one possible assignment):**
  - great     → [1, 0, 0, 0]  (position 0 represents "great")
  - terrible  → [0, 1, 0, 0]  (position 1 represents "terrible")
  - fantastic → [0, 0, 1, 0]  (position 2 represents "fantastic")
  - boring    → [0, 0, 0, 1]  (position 3 represents "boring")

**Key properties:**
- Each vector has exactly one 1 and three 0s
- Each word has a unique pattern
- All vectors are orthogonal to each other (no similarity between words is captured)

References (lectures/practicals used)
- lectures/Lecture 2 - 2025.pdf — p.2 (feature representations overview; basis for one‑hot idea)
- practicals/Practice - W3 - Answer.pdf — p.3 (Label encoding for categorical targets prior to model training)
