![Question 7](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.31.56.png?raw=true)

Question 7 — Role of Backpropagation (matches)

Detailed mapping
- The network's internal parameters (weights and biases): Updated by gradient‑based optimization using gradients returned by backpropagation to reduce the loss.
- The gradients of the loss function: Computed by the backpropagation algorithm (chain rule) with respect to each parameter.
- The learning signal through the network layers: The error signal that is propagated backward from the output layer toward earlier layers, allowing deeper layers to learn.

References (lectures/practicals used)
- lectures/Lecture 2 - 2025.pdf — p.3, p.5 (backpropagation and gradient concepts)
- lectures/Lecture 3-2025.pdf — p.2–3 (training loop and gradient‑based updates)
- lectures/Lecture 4 - 2025.pdf — p.6 (role of gradients/chain rule)
