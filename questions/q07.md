![Question 7](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.31.56.png?raw=true)

Question 7 — Role of Backpropagation (matches)

Detailed explanation

**Understanding Backpropagation:**

Backpropagation is the cornerstone algorithm for training neural networks. It efficiently computes gradients of the loss function with respect to all network parameters using the chain rule of calculus.

**Matching each component:**

**1. The network's internal parameters (weights and biases):**
→ **"Updated by gradient-based optimization"**

- Weights and biases are the learnable parameters
- After backpropagation computes gradients, an optimizer (like SGD or Adam) uses them to update parameters
- Update rule example: w_new = w_old - learning_rate × gradient
- The goal: adjust parameters to minimize the loss function

**2. The gradients of the loss function:**
→ **"Computed by the backpropagation algorithm"**

- Backpropagation computes ∂Loss/∂w for every weight w
- Uses the chain rule to propagate derivatives backwards through the network
- These gradients tell us how much each parameter contributes to the error
- Direction and magnitude information for parameter updates

**3. The learning signal through the network layers:**
→ **"The error signal propagated backward"**

- The error (loss) starts at the output layer
- Backpropagation passes this error backward, layer by layer
- Each layer receives error signals from layers ahead of it
- This allows all layers (including deep/early ones) to learn from the final prediction error
- Solves the "credit assignment problem" - determining which parameters are responsible for errors

**Why Backpropagation is Essential:**

Without backpropagation, we couldn't efficiently train deep networks because:
- Computing gradients by finite differences would be computationally prohibitive
- Deeper layers wouldn't receive meaningful learning signals
- The chain rule allows us to decompose complex derivatives into manageable pieces

References (lectures/practicals used)
- lectures/Lecture 2 - 2025.pdf — p.3, p.5 (backpropagation and gradient concepts)
- lectures/Lecture 3-2025.pdf — p.2–3 (training loop and gradient‑based updates)
- lectures/Lecture 4 - 2025.pdf — p.6 (role of gradients/chain rule)
