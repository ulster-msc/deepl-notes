![Question 5](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.31.45.png?raw=true)

Question 5 — Compile and fit parameters (Keras Sequential)

Completed lines
```python
model.compile(optimizer='adam',
              loss='categorical_crossentropy')

model.fit(Train_x, Train_y,
          epochs=100,
          batch_size=32)
```

Detailed explanation

**Understanding the model architecture:**

The code shows a Sequential model with:
- Input layer: Dense(10) with ReLU activation and He normal initialization, accepting n_features inputs
- Hidden layer: Dense(8) with ReLU activation and He normal initialization
- Output layer: Dense(10) with no activation specified

The final Dense(10) layer suggests a **10-class classification problem**.

**Step-by-step reasoning for compile parameters:**

**1. Choosing the optimizer:**
- **Answer: 'adam'**
- Adam (Adaptive Moment Estimation) is the most widely used optimizer
- Advantages over SGD: adaptive learning rates per parameter, momentum, bias correction
- Works well for most problems without extensive hyperparameter tuning
- Other valid options: 'sgd', 'rmsprop', but 'adam' is the modern default

**2. Choosing the loss function:**
- **Answer: 'categorical_crossentropy'**
- The output layer has 10 units (suggesting 10 classes)
- For multi-class classification (>2 classes), we use categorical crossentropy
- **Important distinction:**
  - `categorical_crossentropy`: Use when labels are one-hot encoded (e.g., [0,0,1,0,...])
  - `sparse_categorical_crossentropy`: Use when labels are integers (e.g., 2)
- The question doesn't specify label format, but 'categorical_crossentropy' is the standard answer
- **Note:** The output layer would typically need `activation='softmax'` added to work properly with this loss

**Step-by-step reasoning for fit parameters:**

**3. Choosing epochs:**
- **Answer: 100** (reasonable default)
- One epoch = one complete pass through the entire training dataset
- 100 is a common starting point for small-medium datasets
- Too few: underfitting; too many: overfitting (mitigated by early stopping)

**4. Choosing batch_size:**
- **Answer: 32** (standard default)
- Batch size determines how many samples are processed before updating weights
- 32 is a common default that balances:
  - Memory efficiency (smaller batches use less memory)
  - Training stability (larger batches give more stable gradients)
  - Convergence speed (moderate batch size often works best)
- Other common choices: 16, 64, 128, 256

References (lectures/practicals used)
- lectures/Lecture 3-2025.pdf — p.3, p.5–6 (compile+loss for multi‑class models with softmax/categorical crossentropy)
- practicals/Practice - W5-r Answers.pdf — p.3 (Keras compile for Iris multi‑class example)
