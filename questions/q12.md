![Question 12](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.32.58.png?raw=true)

Question 12 — MLP output size

- Final fill: **1**

Detailed explanation

**Understanding the question:**

The code shows:
```python
model.add(Dense(10, input_shape=(8,)))
model.add(Dense(___))  # Fill in the number of neurons
```

The second Dense layer is described as having "a Sigmoid activation function" in the output layer.

**Step-by-step reasoning:**

**Step 1: Identify the activation function**
- The output layer uses **Sigmoid** activation
- Sigmoid formula: σ(x) = 1 / (1 + e^(-x))
- Sigmoid range: (0, 1)
- Sigmoid maps any real number to a value between 0 and 1

**Step 2: Determine the task type**
- Sigmoid activation in the output layer is the standard choice for **binary classification**
- Binary classification: predicting one of two classes (e.g., yes/no, spam/not spam, 0/1)
- The sigmoid output can be interpreted as P(class=1)
- Decision rule: if σ(x) ≥ 0.5 → predict class 1, else → predict class 0

**Step 3: Determine the number of output neurons**
- For binary classification with sigmoid: **1 neuron** is sufficient
- This single neuron outputs a probability between 0 and 1
- Why not 2 neurons? Because P(class=0) = 1 - P(class=1), so the second probability is redundant

**Answer: 1**

**Comparison with other scenarios:**
- **Binary classification (2 classes):** 1 neuron with sigmoid, OR 2 neurons with softmax
- **Multi-class (K > 2 classes):** K neurons with softmax
- **Regression:** 1 or more neurons with linear/no activation

**Complete code:**
```python
model.add(Dense(10, input_shape=(8,)))
model.add(Dense(1, activation='sigmoid'))  # Binary classification
```

References (lectures/practicals used)
- lectures/Lecture 2 - 2025.pdf — p.2 (sigmoid properties)
- lectures/Lecture 4 - 2025.pdf — p.3–4 (binary vs. multi‑class outputs; activations)
