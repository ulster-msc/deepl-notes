![Question 13](https://github.com/ulster-msc/deepl-notes/blob/main/questions/Screenshot%202025-11-06%20at%2015.33.03.png?raw=true)

Question 13 — Identify layer names

Detailed explanation

**Understanding CNN Architecture:**

This is a typical Convolutional Neural Network (CNN) for image classification. The code shows the standard pipeline: Convolution → Pooling → Flatten → Dense layers.

**Layer-by-layer identification:**

**1. Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))**
→ **Convolutional Layer (Conv2D)**

- Creates 32 feature maps (filters/kernels)
- Each filter is 3×3 pixels
- Uses ReLU activation
- Input: 28×28 grayscale images (likely MNIST digits)
- Purpose: Extract spatial features like edges, textures

**2. MaxPooling2D((2, 2))**
→ **Max Pooling Layer**

- Pool size: 2×2
- Reduces spatial dimensions by half (28×28 → 14×14 assuming valid padding after conv)
- Keeps the maximum value in each 2×2 region
- Purpose: Downsampling, translation invariance, reduced computation

**3. Flatten()**
→ **Flatten Layer**

- Converts multi-dimensional feature maps into a 1D vector
- Example: 14×14×32 → 6272-dimensional vector
- Purpose: Prepare data for fully connected layers
- No learnable parameters

**4. Dense(64, activation='relu')**
→ **Fully Connected (Dense) Hidden Layer**

- 64 neurons
- ReLU activation
- Fully connected to all neurons from the flattened vector
- Purpose: High-level feature combination and learning

**5. Dense(10, activation='softmax')**
→ **Fully Connected (Dense) Output Layer**

- 10 neurons (for 10 classes, e.g., digits 0-9)
- Softmax activation produces probability distribution
- Sum of outputs = 1.0
- Purpose: Final classification

**Complete architecture summary:**
Input (28×28×1) → Conv2D → MaxPooling → Flatten → Dense(64) → Dense(10) → Output probabilities

References (lectures/practicals used)
- lectures/Lecture 7 - 2025.pdf — p.1–4 (CNN pipeline: Conv2D → MaxPooling2D → Flatten → Dense)
