![Question 1](https://github.com/ulster-msc/deepl-notes/blob/main/questions-2/Screenshot%202025-11-06%20at%2017.07.09.png?raw=true)

Question 1 — Perceptron weighted sum and activation

- Final answers
  - z term: **+ b** (bias)
  - Activation: **step** (threshold) function

Detailed explanation

**Understanding the Perceptron:**

The perceptron is the fundamental building block of neural networks, invented by Frank Rosenblatt in 1958. It computes a weighted sum of inputs and applies a threshold function.

**Step 1: Fill in the missing term in z**

The formula shows:
```
z = (w₁ × x₁) + (w₂ × x₂) + ... + (wₙ × xₙ) + ___
```

The missing term is the **bias (b)**.

**Complete formula:**
```
z = Σ(wᵢ × xᵢ) + b
  = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```

**Why is bias important?**
- Allows the activation function to be shifted left or right
- Without bias, the decision boundary must pass through the origin
- With bias, the model has more flexibility to fit the data
- Similar to the intercept term in linear regression

**Step 2: Identify the activation function**

The question states z is "passed through a(n) ___ function to produce the final output."

For a **simple (classical) Perceptron**, the activation is a **step function** (also called threshold function or Heaviside function):

```
output = step(z) = {
    1  if z ≥ threshold (often 0)
    0  if z < threshold
}
```

**Characteristics of the step function:**
- Binary output: 0 or 1
- Non-differentiable (which limited early perceptrons)
- Creates a hard decision boundary
- Used in the original perceptron for binary classification

**Modern alternatives:**
- Sigmoid: smooth, differentiable, outputs (0, 1)
- ReLU: outputs max(0, x), most common in deep learning
- Tanh: outputs (-1, 1), zero-centered

**Complete perceptron operation:**
1. Compute weighted sum: z = Σ(wᵢxᵢ) + b
2. Apply step function: output = step(z)
3. Result: Binary classification (0 or 1)

References (lectures/practicals used)
- lectures/Lecture 2 - 2025.pdf — p.2 (neuron: linear combination + activation)
- practicals/Pratice -W2 solution.pdf — p.1 (ANN building blocks / activation), p.21 (forward computation with bias)

