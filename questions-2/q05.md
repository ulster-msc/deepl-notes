![Question 5](https://github.com/ulster-msc/deepl-notes/blob/main/questions-2/Screenshot%202025-11-06%20at%2017.07.42.png?raw=true)

Question 5 — Match ANN terms to definitions

Detailed explanation

This question tests understanding of fundamental ANN terminology. Let's match each term with its correct definition:

**1. Epoch**
→ **"One full pass over the entire training dataset"**

- During training, data is processed in batches
- After processing all batches once, one epoch is complete
- Example: If you have 1000 samples and batch size=100, one epoch = 10 batches
- Models typically train for multiple epochs (e.g., 100 epochs)
- More epochs ≠ better (can lead to overfitting)

**2. Bias**
→ **"A trainable offset added to the weighted sum before activation"**

- Mathematical form: z = Σ(wᵢxᵢ) + **b**
- The bias 'b' is a learnable parameter (like weights)
- Allows shifting the activation function
- Essential for model flexibility
- Without bias, decision boundaries must pass through origin

**3. Loss Function**
→ **"A measure of prediction error to be minimized (objective)"**

- Quantifies how wrong the model's predictions are
- Examples:
  - **Mean Squared Error (MSE):** for regression
  - **Categorical Crossentropy:** for multi-class classification
  - **Binary Crossentropy:** for binary classification
- Training seeks to minimize this function
- Also called "cost function" or "objective function"

**4. Optimizer**
→ **"The algorithm that updates weights/biases using gradients"**

- Takes gradients from backpropagation and adjusts parameters
- Common optimizers:
  - **SGD (Stochastic Gradient Descent):** basic, uses learning rate
  - **Adam:** adaptive learning rates, momentum, very popular
  - **RMSprop:** adaptive learning rates
- Update rule example (SGD): w_new = w_old - learning_rate × gradient
- Determines how the model learns

**5. Hidden Layer**
→ **"Any layer between the input and output layers"**

- Not directly visible to the input or output
- Learns intermediate representations/features
- Deep networks have multiple hidden layers
- Example: Input(784) → Hidden(128) → Hidden(64) → Output(10)
- More hidden layers = deeper network = can learn more complex patterns

**Summary of matches:**
1. Epoch → One full pass over the entire training dataset
2. Bias → A trainable offset added to the weighted sum before activation
3. Loss Function → A measure of prediction error to be minimized
4. Optimizer → The algorithm that updates weights/biases using gradients
5. Hidden Layer → Any layer between the input and output layers

References (lectures/practicals used)
- lectures/Lecture 3-2025.pdf — p.5–6 (Keras training loop: compile/fit/evaluate concepts)
- practicals/Practice - W5-r Answers.pdf — p.3 (compile and train settings: optimizer/loss)
- practicals/Pratice -W2 solution.pdf — p.1 (ANN components including bias/hidden layer)

