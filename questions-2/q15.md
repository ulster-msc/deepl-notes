![Question 15](https://github.com/ulster-msc/deepl-notes/blob/main/questions-2/Screenshot%202025-11-06%20at%2017.08.45.png?raw=true)

Question 15 — Activation that mitigates vanishing gradients

- Final answer: **B) ReLU (Rectified Linear Unit)**

Detailed explanation

**Understanding the Vanishing Gradient Problem:**

The vanishing gradient problem is a critical challenge in training deep neural networks:
- During backpropagation, gradients are multiplied layer by layer
- If gradients are < 1, they exponentially decrease in deeper layers
- Deeper layers receive extremely small gradients → very slow learning or no learning
- This prevented training of deep networks before modern solutions

**Why Sigmoid and Tanh cause vanishing gradients:**

**Sigmoid: σ(x) = 1/(1 + e^(-x))**
- Output range: (0, 1)
- **Problem:** Saturates at both ends
- For large |x|, gradient ≈ 0
- Maximum gradient: 0.25 (at x=0)
- Gradients always < 1, get multiplied through many layers → vanish

**Tanh: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))**
- Output range: (-1, 1)
- **Problem:** Similar saturation issues
- Maximum gradient: 1.0 (at x=0)
- Still suffers from vanishing gradients in deep networks

**Why ReLU solves this:**

**ReLU: f(x) = max(0, x)**

```
f(x) = {
    x   if x > 0
    0   if x ≤ 0
}
```

**Gradient:**
```
f'(x) = {
    1   if x > 0
    0   if x ≤ 0
}
```

**Advantages:**
1. **No saturation for positive inputs:** Gradient = 1 for x > 0
2. **Gradients don't diminish:** Multiplying by 1 preserves gradient magnitude
3. **Faster convergence:** Stronger gradient signals
4. **Computational efficiency:** Simple max operation, no expensive exponentials
5. **Sparse activation:** Only some neurons activate (biological plausibility)

**Comparison table:**

| Activation | Range | Max Gradient | Vanishing Gradient? |
|------------|-------|--------------|---------------------|
| Sigmoid | (0, 1) | 0.25 | Yes ✗ |
| Tanh | (-1, 1) | 1.0 | Yes (in deep networks) ✗ |
| **ReLU** | **[0, ∞)** | **1.0** | **No ✓** |
| Linear | (-∞, ∞) | 1.0 | No, but can't learn nonlinear patterns ✗ |
| Step | {0, 1} | 0 (undefined) | Worst - no gradient ✗ |

**Why other options are wrong:**

- **A) Linear:** Has gradient = 1, but can't learn nonlinear functions (useless for hidden layers)
- **C) Step Function:** Has zero gradient everywhere (except at discontinuity where it's undefined)
- **D) Sigmoid:** Actually causes vanishing gradients!

**Answer: B) ReLU**

**Modern variants of ReLU:**
- **Leaky ReLU:** f(x) = max(0.01x, x) - allows small negative gradients
- **PReLU:** Learnable negative slope
- **ELU:** Smooth for negative values
All aim to further improve upon ReLU's benefits.

References (lectures/practicals used)
- lectures/Lecture 2 - 2025.pdf — p.2 (activation functions including ReLU)
- lectures/Lecture 3-2025.pdf — p.5 (deep MLPs commonly using ReLU)

